{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da2f0eb",
   "metadata": {},
   "source": [
    "## 0. Install libraries and enable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cb3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from IPython.core.display import HTML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, roc_curve, auc, confusion_matrix, classification_report, roc_auc_score\n",
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "from bertviz import head_view, model_view\n",
    "from captum.attr import LayerIntegratedGradients, visualization\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "import requests\n",
    "\n",
    "# supress warnings:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dir = '/home/user/Documents/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify CUDA and PyTorch compatibility\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "  \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # set to the GPU ID you want to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a3426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()  # collect garbage\n",
    "torch.cuda.empty_cache() # clear cuda cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97093707",
   "metadata": {},
   "source": [
    "# 1. Load, preprocess and split the data\n",
    "\n",
    "The SMILES will be sourced from the Wolfram Alpha API.\n",
    "We will send the requests to the API for the largest data set (tve).\n",
    "Since many of the substrates are the same for the three laccases, we will join the smiles from the tve data and fill in the gaps by sending requests to the API (it is faster than requesting all information).\n",
    "\n",
    "Split the preprocessed data into training and test sets:\n",
    "* training set: 80%\n",
    "* test set: 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c6a80",
   "metadata": {},
   "source": [
    "### Option 1: Load using Wolfram Alpha\n",
    "\n",
    "\n",
    "```python\n",
    "app_id = '2XHLXX-X33XRX4XXX' # valid Wolfram Alpha API credentials (api key):\n",
    "\n",
    "def get_smiles(ids):\n",
    "    \n",
    "    query = ids + ' SMILES identifier' # query for the chemical\n",
    "    url = f\"http://api.wolframalpha.com/v2/query?input={query}&appid={app_id}&output=json\" # Wolfram Alpha API endpoint\n",
    "    # send the request to the Wolfram Alpha API:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200: # sucsess\n",
    "        data = response.json()\n",
    "        # the API response contains pods, the SMILES string is in one of them:\n",
    "        pods = data.get(\"queryresult\", {}).get(\"pods\", [])\n",
    "        try:\n",
    "            ans = pods[1].get(\"subpods\")[0].get('plaintext')\n",
    "            return ans\n",
    "        except:\n",
    "            return 'Did not work'\n",
    "\n",
    "\n",
    "\n",
    "# load the data, drop duplicates:\n",
    "\n",
    "data_tve = pd.read_csv(os.path.join(dir, 'tve-smiles.csv'), encoding=\"utf-8\", sep=';')\n",
    "ids_tve = data_tve['IUPAC Name'].to_list()\n",
    "smiles_tve = []\n",
    "for ids in ids_tve:\n",
    "    smiles_tve.append(get_smiles(ids))\n",
    "data_tve['SMILES'] = smiles_tve  \n",
    "  \n",
    "# insert the result for N-(2,3,4,5,6-pentahydroxyhexyl)-3-(2,4-trihydroxyphenyl)propanamide manually:\n",
    "data_tve.loc[data_tve['IUPAC Name']=='N-(2,3,4,5,6-pentahydroxyhexyl)-3-(2,4-trihydroxyphenyl)propanamide','SMILES'] = 'C1=CC(=C(C(=C1CCC(=O)NCC(C(C(C(CO)O)O)O)O)O)O)O'\n",
    "data_tve = data_tve.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "data_mth = pd.read_csv(os.path.join(dir, 'mth-smiles.csv'), encoding=\"utf-8\", sep=';')\n",
    "\n",
    "# join the already obtained SMILES and fill in the gaps:\n",
    "data_mth = data_mth.merge(data_tve[['IUPAC Name','SMILES']], how='left', on='IUPAC Name')\n",
    "ids_mth = data_mth[data_mth['SMILES'].isna()]['IUPAC Name'].to_list()\n",
    "\n",
    "if len(ids_mth)!=0:\n",
    "    for ids in ids_mth:\n",
    "        data_mth.loc[data_mth['IUPAC Name']==ids,'SMILES'] = get_smiles(ids)\n",
    "    \n",
    "data_mth['Oxd'].fillna(0, inplace=True) # replace the only MV with 0\n",
    "data_mth['Oxd'] = data_mth['Oxd'].round().astype('int64')\n",
    "\n",
    "data_mth = data_mth.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "data_bpu = pd.read_csv(os.path.join(dir, 'bpu-smiles.csv'), encoding=\"utf-8\", sep=';')\n",
    "data_bpu = data_bpu.merge(data_tve[['IUPAC Name','SMILES']], how='left', on='IUPAC Name')\n",
    "\n",
    "# there should be no NANs, but just in case:\n",
    "ids_bpu = data_bpu[data_bpu['SMILES'].isna()]['IUPAC Name'].to_list()\n",
    "\n",
    "if len(ids_bpu)!=0:\n",
    "    for ids in ids_bpu:\n",
    "        data_bpu.loc[data_bpu['IUPAC Name']==ids,'SMILES'] = get_smiles(ids)\n",
    "        \n",
    "data_bpu = data_bpu.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ensure that each substrate in all sets has recieved a corresponding SMILES string: \n",
    "assert (len(data_tve[data_tve['SMILES']=='Did not work']) + len(data_mth[data_mth['SMILES'].isna()]==True) + len(data_bpu[data_bpu['SMILES'].isna()]==True))==0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b3f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# save the dataframes with SMILES:\n",
    "data_tve.to_csv(os.path.join(dir, 'f-tve-smiles.csv'), index=False, encoding='utf-8', sep=';')\n",
    "data_mth.to_csv(os.path.join(dir, 'f-mth-smiles.csv'), index=False, encoding='utf-8', sep=';')\n",
    "data_bpu.to_csv(os.path.join(dir, 'bpu-lac-smiles.csv'), index=False, encoding='utf-8', sep=';') \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8ca67",
   "metadata": {},
   "source": [
    "### Option 2: Load from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4a2cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tve = pd.read_csv(os.path.join(dir, 'f-tve-smiles.csv'), encoding=\"utf-8\", sep=';')\n",
    "data_mth = pd.read_csv(os.path.join(dir, 'f-mth-smiles.csv'), encoding=\"utf-8\", sep=';')\n",
    "data_bpu = pd.read_csv(os.path.join(dir, 'bpu-lac-smiles.csv'), encoding=\"utf-8\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8ba5a",
   "metadata": {},
   "source": [
    "### Split the data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01c84334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the split ratios and split the dataset into training, validation and test sets:\n",
    "train_data_tve, test_data_tve, train_labels_tve, test_labels_tve = train_test_split(data_tve['SMILES'], data_tve['Oxd'], test_size=0.2, random_state=98765)\n",
    "train_data_mth, test_data_mth, train_labels_mth, test_labels_mth = train_test_split(data_mth['SMILES'], data_mth['Oxd'], test_size=0.2, random_state=9876)\n",
    "train_data_bpu, test_data_bpu, train_labels_bpu, test_labels_bpu = train_test_split(data_bpu['SMILES'], data_bpu['Oxd'], test_size=0.2, random_state=987)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc93fe8",
   "metadata": {},
   "source": [
    "## 2. Fine-tune a pre-trained transformer on the SMILES data\n",
    "\n",
    "We will be using a pre-trained ChemBERTa (RoBERTa trained on SMILES) + binary classification top layer (with a dropout).\n",
    "The tuning of the model parameters (weights and biases) follows via minimizing the adjusted for the class imbalance cross-entropy loss function.\n",
    "\n",
    "We will be selecting the best combination of the following parameters:\n",
    "\n",
    "* `batch_size`\n",
    "* `learning_rate`\n",
    "* `dropout_rate`\n",
    "* `patience`.\n",
    "\n",
    "\n",
    "#### Key Steps for the hyperparameter tuning:\n",
    "* create a custom ChemBERTa model class inheriting from the RoBERTa trained on the `PubChem10M_SMILES_BPE_450k` dataset, which will be using a dropout layer and a binary classification layer on top \n",
    "* define the loop for hyperparameter tuning using the 5-fold CV, which splits the training data into 5 folds, and for each of the 5 splits does the following:\n",
    "    * preprocess SMILES Data (tokenize, create attention masks, convert to pt tensors)\n",
    "    * train the model using the 4 folds and compute average validation accuracy over the 5-th fold.\n",
    "\n",
    "\n",
    "\n",
    "* ensure that finetuning implements early stopping by monitoring validation loss and the loss function is adjusted for the class weights to account for the class imbalance in the data.\n",
    "\n",
    "The best set of parameters will be selected via random choice on a grid using 5-fold CV and the validation accuracy averaged over all epochs.\n",
    "\n",
    "#### Train the final model and predict:\n",
    "* rebuild the model with the best hyperparameters\n",
    "* predict and evaluate on the test set.\n",
    "\n",
    "\n",
    "\n",
    "#### Why combine dropout with early stopping?\n",
    "Dropout is applied during the forward pass in training to regularize the network and helps to prevent overfitting by randomly setting a portion of neurons to zero during training, which forces the model to learn more robust features. \n",
    "\n",
    "Early stopping monitors the model's performance *(we control for F1 score) on the validation set during training and halts training when the validation performance stomps to improve or starts to degrade for a specified number of epochs (often called the patience parameter). It ensures that the model doesn't over-train and minimizes the risk of overfitting beyond the optimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c12400b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRoberta(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout_prob):\n",
    "        \n",
    "        super(CustomRoberta, self).__init__()\n",
    "        self.checkpoint = 'seyonec/PubChem10M_SMILES_BPE_450k' \n",
    "        self.roberta = RobertaModel.from_pretrained(self.checkpoint, output_attentions=True, output_hidden_states=True).to(device)\n",
    "        self.config = self.roberta.config \n",
    "        self.current_embeddings = self.roberta.get_input_embeddings()\n",
    "        self.tokenizer = RobertaTokenizerFast.from_pretrained(self.checkpoint)\n",
    "        self.dropout = nn.Dropout(dropout_prob).to(device)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, 2).to(device)\n",
    "       \n",
    "        \n",
    "        \n",
    "    def prepare_data(self, data, labels, batch_size, shuffle):\n",
    "        \n",
    "        # tokenize the SMILES, extract input ids and attention masks:\n",
    "        tokenized = self.tokenizer(data.tolist(), padding='max_length', truncation=True, max_length=72, return_tensors='pt')    # if we set max_length 72, only one SMILES (len=163) is truncated, but if set max_length to 165, CUDA runs out of memory\n",
    "        input_ids = tokenized['input_ids']\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "        # convert labels to tensors:\n",
    "        labels = torch.tensor(labels.values)\n",
    "        # create DataLoaders for batching:\n",
    "        dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "        \n",
    "        def seed_worker(worker_id):\n",
    "            worker_seed = 42 + worker_id\n",
    "            np.random.seed(worker_seed)\n",
    "            random.seed(worker_seed)\n",
    "        \n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=None, worker_init_fn=seed_worker)\n",
    "        \n",
    "        return loader\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # [CLS] token representation\n",
    "        dropped_output = self.dropout(pooled_output)  # add dropout\n",
    "        logits = self.classifier(dropped_output)  # pass through classification layer\n",
    "        \n",
    "        return logits, outputs.attentions, outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d8c20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds:\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def select_model(trial, smiles, property):\n",
    "    \n",
    "    set_seed(42)\n",
    "     \n",
    "    # defne the tuning grid:\n",
    "    try:\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, step=5e-5) \n",
    "        patience = trial.suggest_int('patience', 6, 12, step=1)\n",
    "        batch_size = trial.suggest_int('batch_size', 8, 64, step=8)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.6, step=0.1)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to suggest hyperparameters: {e}\")\n",
    "    \n",
    "\n",
    "    model = CustomRoberta(dropout_prob=dropout_rate)\n",
    "     \n",
    "    # print trial information:\n",
    "    print(f'Trial {trial.number+1}: lr = {learning_rate}, patience = {patience}, batch_size = {batch_size}, dropout = {dropout_rate}')\n",
    "     \n",
    "    # define class weights for loss adjustment:\n",
    "    class_weights = torch.tensor(\n",
    "                                 compute_class_weight(class_weight='balanced', classes=np.unique(property), y=property.values), \n",
    "                                 dtype=torch.float\n",
    "                                 ).to(device)\n",
    "    \n",
    "    # shuffle indices before splitting the set:\n",
    "    indices = property.index.to_list()\n",
    "    np.random.shuffle(indices)\n",
    "    fold_size = len(property)//5\n",
    "    \n",
    "    fold_f1s = []\n",
    "        \n",
    "    # the 5-CV loop:\n",
    "    for i in range(5):\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        start = i*fold_size\n",
    "        end = (i+1)*fold_size if i!=4 else len(property)\n",
    "        ind = indices[start:end] \n",
    "\n",
    "        # data for the folds and loaders:\n",
    "        valid_dt, valid_labs = smiles[ind], property[ind]\n",
    "        train_dt, train_labs = smiles.drop(ind), property.drop(ind)\n",
    "\n",
    "        train_loader = model.prepare_data(train_dt, train_labs, batch_size, shuffle=True)\n",
    "        valid_loader = model.prepare_data(valid_dt, valid_labs, batch_size, shuffle=False)\n",
    " \n",
    " \n",
    "        # define the optimizer, modified loss and the top layer with dropout:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_val_f1 = 0\n",
    "        patience_counter = 0\n",
    "        train_loss_hist, val_loss_hist, val_f1_hist = [], [], []\n",
    "\n",
    "        # loop over epochs:\n",
    "        for epoch in range(500):\n",
    "\n",
    "            # train:\n",
    "            model.train()\n",
    "            train_tot_loss = 0 # accumulated loss over an epoch\n",
    "            train_tot_samples = 0 # accumulated number of examples over an epoch\n",
    "\n",
    "            for batch in train_loader:\n",
    "\n",
    "                inputs_train, mask_train, labels_train = [b.to(device) for b in batch]\n",
    "                logits_train, _, _ = model.forward(inputs_train, mask_train)\n",
    "                loss_train = loss_fn(logits_train, labels_train)\n",
    "\n",
    "                # backward pass and optimization:\n",
    "                optimizer.zero_grad()\n",
    "                loss_train.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_tot_loss += loss_train.item()  # accum loss over all batches in epoch\n",
    "                train_tot_samples += len(labels_train)  # num examples over all batches in epoch\n",
    "            \n",
    "            train_tot_loss = train_tot_loss / train_tot_samples # avg training loss per example over epoch\n",
    "            train_loss_hist.append(train_tot_loss)\n",
    "            print(f'CV iter {i+1}/5, Epoch {epoch+1} \\nAverage Training Loss: {train_tot_loss}')\n",
    "            scheduler.step() \n",
    "\n",
    "            # validate:\n",
    "            model.eval()\n",
    "            val_tot_loss = 0\n",
    "            val_tot_samples = 0\n",
    "            all_labs, all_probs, all_preds = [], [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader: \n",
    "\n",
    "                    inputs_val, mask_val, labels_val = [b.to(device) for b in batch]\n",
    "                    logits_val, _, _ = model.forward(inputs_val, mask_val)\n",
    "                    loss_val = loss_fn(logits_val, labels_val)\n",
    "                    val_tot_loss += loss_val.item()\n",
    "                    val_tot_samples += len(labels_val)  # num examples over all batches in epoch\n",
    "                    probs_val = torch.sigmoid(logits_val[:, 1]).cpu().numpy() # use logits for the positives\n",
    "                    preds_val = np.array([int(i > 0.5) for i in probs_val])\n",
    "                    all_labs.append(labels_val.cpu().numpy())   \n",
    "                    all_probs.append(probs_val)\n",
    "                    all_preds.append(preds_val)\n",
    "\n",
    "            val_f1 = f1_score(np.concatenate(all_labs), np.concatenate(all_preds)) # validation F1 per epoch\n",
    "            val_loss = val_tot_loss / val_tot_samples # validation loss per example (over epoch)\n",
    "            val_loss_hist.append(val_loss)\n",
    "            val_f1_hist.append(val_f1)\n",
    "\n",
    "            print(f'Average Validation Loss per epoch: {val_loss}')\n",
    "            print(f'Average Validation F1 per epoch: {val_f1}')\n",
    "\n",
    "            # check early stopping condition:\n",
    "            if (val_loss < best_val_loss) or (val_f1 > best_val_f1):  # if either loss decreases or tnr or accuracy increase, continue w/o petience penalty\n",
    "                best_val_loss = val_loss\n",
    "                best_val_f1 = val_f1\n",
    "                patience_counter = 0 # reset patience\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        # after the CV iteration, append the results:        \n",
    "        fold_f1s.append(np.mean(val_f1_hist))\n",
    "\n",
    "    mean_val_f1 = np.mean(fold_f1s)\n",
    "    return mean_val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d6499",
   "metadata": {},
   "source": [
    "### Fine-tune the model via Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(smiles, property, n_trials=75): \n",
    "    study_ = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study_.optimize(lambda trial: select_model(trial, smiles, property), n_trials)\n",
    "    \n",
    "    # print best parameters and best accuracy\n",
    "    print(\"Best hyperparameters:\", study_.best_params)\n",
    "    print(\"Best validation accuracy:\", study_.best_value)\n",
    "    return study_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec7d20",
   "metadata": {},
   "source": [
    "## 3. Rebuild the model using the optimal hyperparameters and make predictions \n",
    "\n",
    "Rebuild the model using the optimal hyperparameter values (without CV, using all available training data) and make predictions on the test data, compute the following metrics:\n",
    "* accuracy, \n",
    "* precision, \n",
    "* recall, \n",
    "* F1 score, \n",
    "* ROC, \n",
    "* AUROC.\n",
    "\n",
    "\n",
    "For the final model, we also visualize the attention (using BertViz) to better understand to which structures the model is paying attention to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f52d7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild the model with the best parameters:\n",
    "def rebuild_model(train_data, train_labels, test_data, test_labels, study):\n",
    "    \n",
    "    set_seed(42)\n",
    "     \n",
    "    best_model = CustomRoberta(dropout_prob=study.best_params['dropout_rate']).to(device)\n",
    "    best_model.config.output_attentions = True\n",
    "    best_model.config.output_hidden_states = True\n",
    "    \n",
    "    train_loader = best_model.prepare_data(train_data, train_labels, batch_size=study.best_params['batch_size'], shuffle=True)\n",
    "    test_loader = best_model.prepare_data(test_data, test_labels, batch_size=len(test_data), shuffle=False)\n",
    "    \n",
    "    # modified loss:    \n",
    "    class_weights = torch.tensor(\n",
    "                                compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels.values),\n",
    "                                dtype=torch.float\n",
    "                                ).to(device)    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.AdamW(best_model.parameters(), lr=study.best_params['learning_rate'])\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "        \n",
    "    best_tloss = float('inf')\n",
    "    best_tf1 = 0\n",
    "    patience = study.best_params['patience']\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # train:\n",
    "    for epoch in range(500):\n",
    "            \n",
    "            best_model.train()\n",
    "            total_loss = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            all_labs, all_probs, all_preds = [], [], []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                inputs_train, mask_train, labels = [b.to(device) for b in batch]\n",
    "\n",
    "                # forward pass:\n",
    "                logits, _, _ = best_model(input_ids=inputs_train, attention_mask=mask_train)\n",
    "                loss = loss_fn(logits, labels)             \n",
    "                \n",
    "                # backward pass and optimization:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_samples += len(labels)\n",
    "                probs = torch.sigmoid(logits[:, 1]).detach().cpu().numpy() # use logits for the positives\n",
    "                preds = np.array([int(i > 0.5) for i in probs])\n",
    "                all_labs.append(labels.cpu().numpy())   \n",
    "                all_probs.append(probs)\n",
    "                all_preds.append(preds)\n",
    "\n",
    "            train_f1 = f1_score(np.concatenate(all_labs), np.concatenate(all_preds)) # validation loss per epoch\n",
    "            train_loss = total_loss / total_samples # avg training loss per example over epoch\n",
    "            \n",
    "            \n",
    "            print(f'Epoch {epoch+1}, Training Loss: {train_loss}, Training F1: {train_f1}')\n",
    "            scheduler.step() \n",
    "            \n",
    "            # check early stopping condition:\n",
    "            if (train_loss < best_tloss) or (train_f1 > best_tf1):\n",
    "                best_tloss = train_loss\n",
    "                best_tf1 = train_f1\n",
    "                patience_counter = 0  # reset patience\n",
    "                \n",
    "                    \n",
    "                if os.path.exists(os.path.join(dir, 'best_model.pth')):\n",
    "                    os.remove(os.path.join(dir, 'best_model.pth'))\n",
    "                else:\n",
    "                    print(f\"{dir}/best_model.pth does not exist and will be created.\")\n",
    "                torch.save(best_model.state_dict(), os.path.join(dir, 'best_model.pth'))  # save the best model\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "\n",
    "    # restore the best weights and evaluate on the hold-out set:\n",
    "    best_model.load_state_dict(torch.load(os.path.join(dir, 'best_model.pth'), map_location=device))\n",
    "\n",
    "    # test:\n",
    "    best_model.eval()\n",
    "    logits_, probs_, true_labels = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in test_loader:  \n",
    "            \n",
    "            inputs, mask, labels = [b.to(device) for b in batch]\n",
    "            logits, attentions, h_states = best_model.forward(inputs, mask)\n",
    "            probs = torch.sigmoid(logits[:, 1]).cpu().numpy()  # use the logits for the 2nd class (positive)          \n",
    "\n",
    "            logits_.append(logits.cpu().numpy())\n",
    "            probs_.append(probs)\n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "    logits_ = np.concatenate(logits_)\n",
    "    probs_ = np.concatenate(probs_)\n",
    "    true_labels = np.concatenate(true_labels)        \n",
    "    \n",
    "    return best_model, logits_, attentions, h_states, inputs, mask, probs_, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbc5b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "from sklearn.metrics import auc\n",
    "def plot_roc(fpr, tpr, cm):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    \n",
    "    # calculate ROC AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot the ROC curve using matplotlib's ax.plot\n",
    "    ax1.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc, color=[0.72110727, 0.11649366, 0.2828143, 0.85])\n",
    "    ax1.fill_between(fpr, 0, tpr, alpha=0.2, color=[0.95686275, 0.42745098, 0.2627451, 0.85])  # auc shading\n",
    "    ax1.plot([0, 1], [0, 1], linestyle='--', color=[0.28742791, 0.41499423, 0.68512111, 0.85])  # random classifier\n",
    "    \n",
    "    # adjust ticks, labels, and title\n",
    "    ax1.tick_params(axis='both', labelsize=8)\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=9)\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=9)\n",
    "    ax1.set_title('ROC Curve, AUC = %0.2f' % roc_auc, fontsize=11)\n",
    "    ax1.legend(loc='lower right')\n",
    "    \n",
    "    # plot the confusion matrix heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlBu_r', alpha=0.85, annot_kws={\"fontsize\":8}, ax=ax2) # to remove the colorbar, add: cbar=False\n",
    "    cbar = ax2.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=8)\n",
    "    ax2.tick_params(axis='both', labelsize=8)\n",
    "    ax2.set_xlabel('Predicted', fontsize=9)\n",
    "    ax2.set_ylabel('True', fontsize=9)\n",
    "    ax2.set_title('Confusion Matrix', fontsize=11)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# compute metrics:\n",
    "def evaluate_model(true_labels, probs, threshold=0.5):\n",
    "    \n",
    "    predictions = [int(i > threshold) for i in probs] \n",
    "    true_labels = [int(i) for i in true_labels]  # ensure labels are integers\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    # ROC and AUC:\n",
    "    fpr, tpr, thresholds = roc_curve(np.array(true_labels), np.array(probs))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}, AUC: {roc_auc}')\n",
    "    \n",
    "    # compute the confusion matrix:\n",
    "    cm = confusion_matrix(np.array(true_labels), predictions)\n",
    "    # print the classification report:\n",
    "    report = classification_report(np.array(true_labels), predictions, target_names=['Class 0', 'Class 1'])\n",
    "    print(report)\n",
    "    \n",
    "    plot_roc(fpr, tpr, cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d5a04",
   "metadata": {},
   "source": [
    "## 4. Example use\n",
    "Fine-tune, rebuild with optimized parameters and evaluate (for the f-tve data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optuna study and optimize\n",
    "study_tve = tune_model(train_data_tve, train_labels_tve)\n",
    "final_model_tve, logits_tve, attentions_tve, h_states_tve, inputs_tve, mask_tve, probs_tve, true_labels_tve = rebuild_model(train_data_tve, train_labels_tve, test_data_tve, test_labels_tve, study_tve)\n",
    "evaluate_model(true_labels_tve, probs_tve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92a23a",
   "metadata": {},
   "source": [
    "## 5. Attention and LIG analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a71212",
   "metadata": {},
   "source": [
    "### Attention Visualization (BertViz)\n",
    "\n",
    "\n",
    "By setting `idx=14` we choose the 15th sample in the test set.\n",
    "\n",
    "\n",
    "```python\n",
    "# tokenize\n",
    "\n",
    "def tokenize(smiles): \n",
    "    return tokenizer(smiles, padding=True, truncation=True, max_length=180, return_tensors='pt').to(device)\n",
    "\n",
    "```\n",
    "\n",
    "#### Key Aspects of `head_view`:\n",
    "\n",
    "\n",
    "* Transformers have multiple attention heads in each layer. Each head learns to focus on different parts of the input text, capturing distinct patterns or relationships (e.g., coreference, syntax, or semantic connections). The head_view allows you to see the output of each head in every layer, displaying what each head \"attends to.\"\n",
    "\n",
    "* It visualizes the attention scores, which indicate how much each token attends to every other token in the input sequence. These scores are typically represented as heatmaps, with intensity showing the magnitude of attention.\n",
    "\n",
    "* You can inspect specific layers (e.g., layer 3, head 5) to see attention patterns at different stages of processing. Early layers may focus on local and syntactic relationships, while later layers often capture broader, more semantic dependencies.\n",
    "\n",
    "* The tokens of the input sequence are displayed along both axes of the attention heatmap.\n",
    "Rows correspond to query tokens (tokens being \"attended to\"), while columns represent key tokens (tokens \"providing attention\").\n",
    "This setup lets you see which tokens each query token attends to most strongly.\n",
    "\n",
    "\n",
    "\n",
    "#### Practical Insights from `head_view`:\n",
    "\n",
    "* Interpretation of Model Behavior:\n",
    "By examining which tokens a specific token attends to, you can infer how the model processes relationships (e.g., syntactic dependencies like subject-verb or coreferences in pronouns).\n",
    "\n",
    "* Understanding Linguistic Representation:\n",
    "`head_view` reveals how attention is distributed, offering a window into how the model captures semantic and syntactic features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94134acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_attentions(idx, inputs, mask, attentions):\n",
    "\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k')\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in inputs.tolist()]\n",
    "    tokens = np.array(tokens[idx]) # tokens for the selected sample\n",
    "\n",
    "    # truncate to content (remove <pad>):\n",
    "    msk = mask[idx].cpu().numpy()\n",
    "    tokens_truncated = tokens[msk!=0]\n",
    "    extended_mask = mask.unsqueeze(1).unsqueeze(2) # extend the mask for attention computation\n",
    "    masked_attentions = [attn * extended_mask for attn in attentions]\n",
    "\n",
    "    # slice attention tensors for the selected example (idx = ...)\n",
    "    sliced_attentions = []\n",
    "    for attn in masked_attentions: # iterate over layers\n",
    "        # extract attention for the selected idx only:\n",
    "        attn_idx = attn[idx].cpu() # shape: [num_heads, seq_len, seq_len]\n",
    "        l = int(msk.sum())\n",
    "        sliced_attentions.append(attn_idx[:, :l, :l])\n",
    "\n",
    "\n",
    "    # stack attention tensors for BERTViz:\n",
    "    selected_attentions = tuple([layer_attn.unsqueeze(0) for layer_attn in sliced_attentions])\n",
    "    return head_view(selected_attentions, tokens_truncated, html_action='return') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2112cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_attentions(14, inputs_tve, mask_tve, attentions_tve) # 15-th example in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0bf91",
   "metadata": {},
   "source": [
    "### Visualize attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057eb071",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "num_heads = 12\n",
    "idx = 14\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k')\n",
    "tokens_tve = [tokenizer.convert_ids_to_tokens(ids) for ids in inputs_tve.tolist()]\n",
    "tokens_tve = np.array(tokens_tve[idx]) # tokens for the selected sample\n",
    "msk = mask_tve[idx].cpu().numpy()\n",
    "tokens_tve = tokens_tve[msk!=0]\n",
    "seq_len = len(tokens_tve)\n",
    "\n",
    "def plot_attention_weights(layer_idx, head_idx):\n",
    "    # Extract the attention weights for the specific layer and head\n",
    "    attention_weights = attentions_tve[layer_idx][idx][head_idx].cpu().detach().numpy()\n",
    "    attention_weights = attention_weights[:seq_len, :seq_len]\n",
    "    # Plot the heatmap for the 1st layer and 1st attention head:\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(attention_weights, cmap='Spectral_r', cbar_kws={'shrink': 0.8})\n",
    "    plt.title(f\"Attention Weights - Layer {layer_idx + 1}, Head {head_idx + 1}\", fontsize=12)\n",
    "    plt.xlabel('Tokens', fontsize=10)\n",
    "    plt.ylabel('Tokens', fontsize=10)\n",
    "    plt.xticks(ticks=range(seq_len), labels=tokens_tve, fontsize=8, rotation=90)\n",
    "    plt.yticks(ticks=range(seq_len), labels=tokens_tve, fontsize=8, rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create sliders for selecting layer and head\n",
    "layer_slider = widgets.IntSlider(\n",
    "    value=0,  # Default value\n",
    "    min=0,\n",
    "    max=num_layers - 1,\n",
    "    step=1,\n",
    "    description='Layer:',\n",
    "    continuous_update=False  # Update only on release\n",
    ")\n",
    "\n",
    "head_slider = widgets.IntSlider(\n",
    "    value=0,  # Default value\n",
    "    min=0,\n",
    "    max=num_heads - 1,\n",
    "    step=1,\n",
    "    description='Head:',\n",
    "    continuous_update=False  # Update only on release\n",
    ")\n",
    "\n",
    "# Bind sliders to the plotting function using `interactive`\n",
    "interactive_plot = widgets.interactive(\n",
    "    plot_attention_weights,\n",
    "    layer_idx=layer_slider,\n",
    "    head_idx=head_slider\n",
    ")\n",
    "\n",
    "# Display the sliders and plot\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56620e55",
   "metadata": {},
   "source": [
    "### Visualize integrated gradients *(f-tve example)\n",
    "\n",
    "Define a generic function that generates attributions for each substrate (smiles) oxidation result and stores them in a list using `VisualizationDataRecord` class. \n",
    "\n",
    "In the `Captum.LayerIntegratedGradients` function, the target parameter refers to the output of the model that you want to compute the attributions for. Typically, this would be the index of the class label you're interested in, especially in a classification model. However, it does not necessarily have to be the true label.\n",
    "\n",
    "For Classification Tasks:\n",
    "\n",
    "If you're trying to explain the model's decision for a particular predicted class, you would set the target parameter to the index of that class (e.g., target=predicted_label).\n",
    "If you want to compute attributions with respect to the true class (e.g., for post hoc analysis or debugging), you would set target=true_label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b0e7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret(model, tokenized_smiles, mask, logits, target_label):\n",
    "    \n",
    "    def wrapped_model(inputs, masks):\n",
    "        return model(inputs, masks)[0]\n",
    "    \n",
    "    # Pool out the embeddings. Which answers: Which token embeddings contributed to this prediction? \n",
    "    # The attributions visualizations are per-token\n",
    "    lig = LayerIntegratedGradients(wrapped_model, model.roberta.embeddings) # args: (model, model.embedding)\n",
    "    \n",
    "    model.eval()  \n",
    "    model.zero_grad()\n",
    "\n",
    "    input_indices = tokenized_smiles.unsqueeze(0) if tokenized_smiles.dim() == 1 else tokenized_smiles\n",
    "    mask = mask.unsqueeze(0) if mask.dim() == 1 else mask\n",
    "    # generate reference indices for each sample\n",
    "    pad_token_id = model.tokenizer.pad_token_id\n",
    "    reference_indices = torch.full_like(input_indices, pad_token_id)\n",
    "\n",
    "\n",
    "    logits = logits\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    attributions, delta = lig.attribute(inputs=input_indices, \n",
    "                                        baselines=reference_indices,             \n",
    "                                        target=target_label,\n",
    "                                        additional_forward_args=mask,\n",
    "                                        n_steps=50, \n",
    "                                        return_convergence_delta=True)\n",
    "\n",
    "    return attributions, delta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vis_data_records = []    \n",
    "\n",
    "def add_attribs_to_vis(attribs, tokens, probs, pred_label, true_label, delta, vis_data_rec):\n",
    "    attribs = attribs.sum(dim=2).squeeze(0)\n",
    "    # attribs = attribs / torch.norm(attribs) # omit normalization for now\n",
    "    attribs = attribs.cpu().detach().numpy()\n",
    "\n",
    "    smiles_restored = re.search('<s>(.*)</s>', ''.join(tokens))\n",
    "    smiles_restored = smiles_restored.group(1)\n",
    "    # remove <s>, </s> from attribs\n",
    "    attribs = attribs[1:-1]\n",
    "    attribs = attribs / np.max(np.abs(attribs))  # normalize to [-1, 1]\n",
    "    \n",
    "    prob_pos = probs[1]#torch.sigmoid(logits[1]).cpu().numpy()\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_rec.append(visualization.VisualizationDataRecord(\n",
    "                            attribs,\n",
    "                            prob_pos,\n",
    "                            pred_label,\n",
    "                            true_label,\n",
    "                            1, # this is to denote that we are predicting the probability of the \"Oxd=1\" event\n",
    "                            attribs.sum(),\n",
    "                            smiles_restored,\n",
    "                            delta))\n",
    "    return vis_data_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "303d002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "preds_tve = [int(i > threshold) for i in probs_tve] \n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k')\n",
    "tokens_tve = [tokenizer.convert_ids_to_tokens(ids) for ids in inputs_tve.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307688b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_vis_data_records = []  # store only successful cases\n",
    "\n",
    "for i in range(len(preds_tve)):\n",
    "    attributions_tve, delta_tve = interpret(final_model_tve, inputs_tve[i], mask_tve[i], probs_tve[i], target_label=preds_tve[i])\n",
    "    try:\n",
    "        match = re.search(r'<s>(.*)</s>', ''.join(tokens_tve[i]))\n",
    "        if not match:\n",
    "            print(f\"[Skipping] SMILES extraction failed for index {i}\")\n",
    "            continue\n",
    "        smiles_restored = match.group(1)\n",
    "        \n",
    "        substrate_info = data_tve.loc[data_tve['SMILES'] == smiles_restored, 'Substrate Name']\n",
    "        if substrate_info.empty:\n",
    "            print(f\"[Skipping] No substrate found for SMILES at index {i}\")\n",
    "            continue\n",
    "        \n",
    "        # ensure word-attribution length matches:\n",
    "        if len(tokens_tve[i]) != len(attributions_tve.squeeze().tolist()):\n",
    "            print(f\"[Skipping] Length mismatch at index {i}: tokens={len(tokens_tve[i])}, attributions={len(attributions_tve.squeeze().tolist())}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        add_attribs_to_vis(attributions_tve, tokens_tve[i], logits_tve[i], preds_tve[i], true_labels_tve[i], delta_tve, valid_vis_data_records)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Exception for case {i}: {str(e)}')\n",
    "        continue\n",
    "    \n",
    "    \n",
    "print(\"Visualizing attributions (Integrated Gradients)\")\n",
    "if valid_vis_data_records:\n",
    "    _ = visualization.visualize_text(valid_vis_data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aebf86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smiles_attribs(tokens, masks, attributions, vis_data_records, data):\n",
    "    \"\"\"\n",
    "    Extracts the attributions for each character in the SMILES string.\n",
    "    \n",
    "    Args:\n",
    "        smiles (str): The SMILES string.\n",
    "        attributions (list): The list of attribution values corresponding to each character in the SMILES string.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary mapping each character in the SMILES string to its attribution value.\n",
    "    \"\"\"\n",
    "\n",
    "    smiles_list, names_list, attr_list, tokens_list = [], [], [], []\n",
    "\n",
    "    \n",
    "    # The visualization function internally processes attributions\n",
    "    attributions_for_vis = []\n",
    "    for record in valid_vis_data_records:\n",
    "    # This is the data format viz functions use\n",
    "        attributions_for_vis.append(record.word_attributions)\n",
    "            \n",
    "            \n",
    "    for i in range(len(tokens)):\n",
    "\n",
    "        try:\n",
    "            attrs = attributions_for_vis[i]  \n",
    "            msk = masks[i].cpu().numpy()\n",
    "\n",
    "\n",
    "            # Step 1: Apply mask to ignore padding\n",
    "            valid_indices = np.where(msk == 1)[0]\n",
    "            masked_attributions = attrs[valid_indices]\n",
    "            masked_tokens = [tokens[i][z] for z in valid_indices]\n",
    "\n",
    "            # Step 2: Filter non-zero attributions and special tokens\n",
    "            non_zero_indices = non_zero_indices = np.where(\n",
    "                (masked_attributions != 0) & \n",
    "                (~np.isin(masked_tokens, ['<s>', '</s>'])))[0]\n",
    "            attr = [masked_attributions[j] for j in non_zero_indices]\n",
    "            tckn = [masked_tokens[j] for j in non_zero_indices]\n",
    "\n",
    "            # Step 3: Combine tokens and attributions\n",
    "            smiles_restored = re.search('<s>(.*)</s>', ''.join(tokens[i]))\n",
    "            smiles_restored = smiles_restored.group(1)\n",
    "            subname = data.loc[data['SMILES'] == smiles_restored]['Substrate Name'].to_numpy()[0]\n",
    "\n",
    "            smiles_list.append(smiles_restored)\n",
    "            names_list.append(subname)\n",
    "            attr_list.append(attr)\n",
    "            tokens_list.append(tckn)\n",
    "        except Exception as e:\n",
    "            print(f'Error processing index {i}: {str(e)}')\n",
    "            continue\n",
    "    return smiles_list, names_list, attr_list, tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c2a17",
   "metadata": {},
   "source": [
    "The error below just means that one or several SMILES, which soes not hinder the results rendering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e779c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list_tve, names_list_tve, attr_list_tve, tokens_list_tve = get_smiles_attribs(tokens_tve, mask_tve, attributions_tve, valid_vis_data_records, data_tve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8eeba586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_to_character_level(tokens, attributions):\n",
    "    \"\"\"\n",
    "    Convert token-level attributions to character-level, preserving:\n",
    "    - Multi-character chemical elements (Cl, Br, etc.)\n",
    "    - Special symbols (parentheses, brackets)\n",
    "    - While expanding other multi-character tokens\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of tokens (e.g., ['CC', 'Cl', '(', 'Br', ...])\n",
    "        attributions: List of attribution values (same length as tokens)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (expanded_elements, expanded_attributions)\n",
    "    \"\"\"\n",
    "    # Elements to keep as single tokens\n",
    "    multi_elements = {'Cl', 'Br', 'Si', 'Se', 'Te', 'As', 'Pt', \n",
    "                     'Fe', 'Al', 'Zn', 'Li', 'Na', 'Ca', 'Ba', \n",
    "                     'Cu', 'Ag', 'Au', 'Sn', 'Mg'}\n",
    "    \n",
    "    # Special symbols to keep as single tokens\n",
    "    special_symbols = {'(', ')', '[', ']', '=', '#', '@', '/', '\\\\', \n",
    "                      '-', '+', ':', '.', '%', '0', '1', '2', '3', \n",
    "                      '4', '5', '6', '7', '8', '9'}\n",
    "    \n",
    "    expanded_elements = []\n",
    "    expanded_attrs = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        \n",
    "        # Case 1: Multi-character element (e.g., 'Cl')\n",
    "        if token in multi_elements:\n",
    "            expanded_elements.append(token)\n",
    "            expanded_attrs.append(attributions[i])\n",
    "            i += 1\n",
    "        \n",
    "        # Case 2: Special symbol (keep as single token)\n",
    "        elif token in special_symbols:\n",
    "            expanded_elements.append(token)\n",
    "            expanded_attrs.append(attributions[i])\n",
    "            i += 1\n",
    "        \n",
    "        # Case 3: Multi-character token that should be expanded (e.g., 'CCCC')\n",
    "        else:\n",
    "            for char in token:\n",
    "                # Check if character is part of a multi-element (like 'C' in 'Cl')\n",
    "                if i+1 < len(tokens) and char + tokens[i+1] in multi_elements:\n",
    "                    combined = char + tokens[i+1]\n",
    "                    expanded_elements.append(combined)\n",
    "                    expanded_attrs.append(attributions[i+1])\n",
    "                    i += 2  # Skip next token since we've handled it\n",
    "                else:\n",
    "                    expanded_elements.append(char)\n",
    "                    expanded_attrs.append(attributions[i])\n",
    "            i += 1\n",
    "    \n",
    "    return np.array(expanded_elements), np.array(expanded_attrs)\n",
    "\n",
    "\n",
    "\n",
    "expanded_tokens_list_tve = []\n",
    "expanded_attrs_list_tve = []\n",
    "for i in range(len(tokens_tve)):\n",
    "    try:\n",
    "        expanded_tokens_tve, expanded_attrs_tve = expand_to_character_level(tokens_list_tve[i], attr_list_tve[i])\n",
    "        expanded_tokens_list_tve.append(expanded_tokens_tve)\n",
    "        expanded_attrs_list_tve.append(expanded_attrs_tve)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing index {i}: {str(e)}')\n",
    "        continue  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multiple_smiles_attributions(smiles_list, attributions_list, substrate_labels=None, figsize=(10, 14)):\n",
    "    \"\"\"\n",
    "    Visualize attributions for multiple SMILES strings with substrate labels on the right.\n",
    "    \n",
    "    Args:\n",
    "        smiles_list: List of SMILES token lists (e.g., [['CC', '1', ...], ['C', '=', 'O', ...]])\n",
    "        attributions_list: List of attribution lists (same length as smiles_list)\n",
    "        titles: Optional list of titles for each SMILES (default: \"SMILES 1\", \"SMILES 2\", ...)\n",
    "        substrate_labels: Optional list of labels to display on the right (e.g., [\"Substrate A\", \"Substrate B\"])\n",
    "        figsize: Figure size (width, height)\n",
    "    \"\"\"\n",
    "    n_smiles = len(smiles_list)\n",
    "    \n",
    "    '''\n",
    "    fig, axes = plt.subplots(n_smiles, 1, figsize=figsize, squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "    '''\n",
    "    # Create figure with adjusted layout\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    #gs = plt.GridSpec(n_smiles + 1, 1, height_ratios=[1]*n_smiles + [0.8])  # Last row for colorbar\n",
    "    \n",
    "    gs = plt.GridSpec(n_smiles, 1)  # Only allocate rows for SMILES\n",
    "    axes = [fig.add_subplot(gs[i]) for i in range(n_smiles)]\n",
    "    \n",
    "    cbar_width=0.45\n",
    "    cbar_ax = fig.add_axes([0.05, 0.01, cbar_width, 0.01]) \n",
    "    #cbar_ax = fig.add_subplot(gs[-1])  # Dedicated axis for colorbar\n",
    "    \n",
    "    if substrate_labels is None:\n",
    "        substrate_labels = [''] * n_smiles  # Default to no label\n",
    "    \n",
    "    # Normalize all attributions together for consistent coloring\n",
    "    all_attributions = np.concatenate(attributions_list)\n",
    "    norm = plt.Normalize(vmin=min(all_attributions), vmax=max(all_attributions))\n",
    "    \n",
    "\n",
    "    cmap = plt.cm.RdYlBu  # Red for positive, Blue for negative\n",
    "    for idx, (tokens, attributions, label) in enumerate(zip(smiles_list, attributions_list, substrate_labels)):\n",
    "        ax = axes[idx]\n",
    "        x_pos = 0\n",
    "        \n",
    "        # Plot colored SMILES tokens\n",
    "        for token, attr in zip(tokens, attributions):\n",
    "            color = cmap(norm(attr))\n",
    "            ax.text(x_pos, 0.5, token, fontsize=10, ha='left', va='center',\n",
    "                    bbox=dict(boxstyle='round,pad=0.1', facecolor=color, alpha=0.85))\n",
    "            x_pos += len(token) * 0.02 + 0.02  # Adjust spacing based on token length\n",
    "        \n",
    "        # Add substrate label on the right\n",
    "        ax.text(x_pos + 0.5, 0.5, label, fontsize=9, ha='center', va='center', color='black')\n",
    "        ax.set_xlim(-0.05, x_pos + 1.0)  # Extra space for the label\n",
    "        ax.set_ylim(0, 0.65)\n",
    "        ax.axis('off')\n",
    "        #ax.set_title(title)\n",
    "    # Add a single shared colorbar at the bottom\n",
    "    \n",
    "    # Add colorbar to dedicated axis\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    \n",
    "   \n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax, orientation='horizontal', label='Attribution Score')\n",
    "    cbar.ax.tick_params(labelsize=9)  # Smaller font for colorbar\n",
    "      # Adjust colorbar size\n",
    "\n",
    "    fig.suptitle(\"SMILES Attributions\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.06)\n",
    "    plt.show()\n",
    "\n",
    "visualize_multiple_smiles_attributions(\n",
    "    expanded_tokens_list_tve, \n",
    "    expanded_attrs_list_tve, \n",
    "    names_list_tve,\n",
    "    figsize=(12, 14)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e64d2c",
   "metadata": {},
   "source": [
    "#### Mapping attributions from SMILES to the molecular graph \n",
    "\n",
    "The code below attempts to approximate the mapping of SMILES attributions onto their corresponding substructures in the molecular\n",
    "graph. While this mapping is not perfectly precise, it offers a valuable visual aid for identifying which chemical motifs the\n",
    "model considers influential in determining whether a substrate can be oxidized by the given laccase.\n",
    "\n",
    "\n",
    "\n",
    "##### 1. SMILES Tokenization & Mapping to Molecular Graph\n",
    "\n",
    "**Function:** `smiles_to_mol_mapping()`\n",
    " - maps SMILES characters to RDKit atoms/bonds.\n",
    "\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "* RDKit Molecule Creation:\n",
    "\n",
    "```python\n",
    "mol = Chem.MolFromSmiles(smiles)  # Converts SMILES to RDKit Mol object\n",
    "```\n",
    "\n",
    "* Character-to-Atom Mapping (highlights):\n",
    "    - Bracketed Atoms (e.g., [Na]): Treated as single tokens.\n",
    "    - Alphabetic Characters (e.g., C, N): Mapped to atoms, handling two-letter symbols (e.g., Cl).\n",
    "    - Bonds (=, #, etc.): Tracked but not directly mapped to RDKit bonds yet.\n",
    "    - Branches (()) and Rings (digits): Handled via stack-based parsing.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "* `char_to_atom`: Dict mapping SMILES character positions to RDKit atom indices.\n",
    "* `char_to_bond`: Dict mapping SMILES positions to bond symbols (e.g., =, #).\n",
    "\n",
    "##### 2. Attribution Score Processing\n",
    "\n",
    "**Function:** `score_to_color()`\n",
    " - normalizes scores and assign colors (red = negative, blue = positive).\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "* Robust Normalization: Uses 1st/99th percentiles to minimize outlier effects:\n",
    "\n",
    "```python\n",
    "vmin, vmax = np.percentile(all_scores_flat, [1, 99])\n",
    "```\n",
    "\n",
    "* Colormap: `RdYlBu` (Red-Yellow-Blue) for intuitive visualization.\n",
    "\n",
    "\n",
    "##### 3. Mapping Attributions to Molecular Substructures\n",
    "\n",
    "**Function:** `visualize_attribution_scores_improved()`\n",
    "- aggregates token-level scores to atoms/bonds in the molecular graph.\n",
    "\n",
    "* Atom-Level Mapping:\n",
    "For each atom, average scores of all SMILES characters mapped to it (score aggregation):\n",
    "\n",
    "```python\n",
    "atom_scores[atom_idx].append(attribution_scores[char_pos])\n",
    "```\n",
    "\n",
    "* Thresholding: Ignores scores below min_threshold to reduce noise.\n",
    "\n",
    "* Bond-Level Mapping (Complex):\n",
    "    - Challenge: SMILES bond symbols (e.g., =) dont directly correspond to RDKit bond indices.\n",
    "    - Workaround: Checks if a bond character lies between two atoms in the SMILES string:\n",
    "\n",
    "```python\n",
    "if min(bpos, epos) < char_pos < max(bpos, epos):\n",
    "    bond_highlights[bond.GetIdx()] = ...\n",
    "```\n",
    "\n",
    "##### 4. Visualization\n",
    "\n",
    "**Functions:** `visualize_attribution_scores_improved()`, `create_smiles_attribution_comparison()`\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* SMILES String: Colored by character-level attributions (left panel).\n",
    "* Molecular Graph: Atoms/bonds highlighted based on aggregated scores (right panel).\n",
    "\n",
    "\n",
    "**RDKit Rendering:**\n",
    "\n",
    "* Uses `MolDraw2DCairo` for high-resolution rendering.\n",
    "* Atom highlights are circles with radii scaled by score magnitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5710e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "\n",
    "def score_to_color(score, all_scores, alpha=0.85):\n",
    "    \"\"\"Convert attribution score to color with proper normalization\"\"\"\n",
    "    if all_scores is not None and len(all_scores) > 0:\n",
    "        # Use more robust normalization\n",
    "        '''\n",
    "        all_scores_flat = np.array(all_scores).flatten()\n",
    "        all_scores_flat = all_scores_flat[~np.isnan(all_scores_flat)]  # Remove NaN\n",
    "        '''\n",
    "        all_scores_flat = []\n",
    "        for item in expanded_attrs_list_tve:\n",
    "            all_scores_flat.extend(item)\n",
    "\n",
    "        all_scores_flat = np.array(all_scores_flat)\n",
    "        \n",
    "        if len(all_scores_flat) > 0:\n",
    "            vmin, vmax = np.percentile(all_scores_flat, [1, 99])  # Use percentiles to avoid outliers\n",
    "            if vmax == vmin:\n",
    "                vmax = vmin + 1e-6\n",
    "            \n",
    "            # Normalize score\n",
    "            \n",
    "            norm_score = (score - vmin) / (vmax - vmin)\n",
    "            norm_score = np.clip(norm_score, -1, 1)\n",
    "            \n",
    "            # Use RdYlBu colormap: Red for negative, Yellow for neutral, Blue for positive\n",
    "            cmap = plt.cm.RdYlBu  # Reverse so red is negative, blue is positive\n",
    "            return cmap(norm_score)\n",
    "    \n",
    "    return (0.8, 0.8, 0.8, alpha)  # Default gray\n",
    "\n",
    "def smiles_to_mol_mapping(smiles):\n",
    "    \"\"\"\n",
    "    Improved mapping from SMILES characters to molecular structure\n",
    "    Returns cleaner mappings with better error handling\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol:\n",
    "        raise ValueError(f\"Invalid SMILES string: {smiles}\")\n",
    "    \n",
    "    # Get atom mapping using RDKit's built-in functionality\n",
    "    atom_mapping = {}\n",
    "    bond_char_mapping = {}\n",
    "    \n",
    "    # Use RDKit's SMILES parser to get atom indices\n",
    "    # This is more reliable than manual parsing\n",
    "    try:\n",
    "        # Create a mapping by parsing the SMILES step by step\n",
    "        mol_with_idx = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        # Simple character-to-atom mapping\n",
    "        char_to_atom = {}\n",
    "        char_to_bond = {}\n",
    "        \n",
    "        # Track current position in SMILES\n",
    "        i = 0\n",
    "        atom_idx = 0\n",
    "        stack = []  # For branch tracking\n",
    "        \n",
    "        while i < len(smiles):\n",
    "            char = smiles[i]\n",
    "            \n",
    "            # Handle bracketed atoms [...]\n",
    "            if char == '[':\n",
    "                bracket_start = i\n",
    "                bracket_end = smiles.find(']', i) + 1\n",
    "                if bracket_end > bracket_start:\n",
    "                    for j in range(bracket_start, bracket_end):\n",
    "                        char_to_atom[j] = atom_idx\n",
    "                    i = bracket_end\n",
    "                    atom_idx += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "                    \n",
    "            # Handle regular atoms (including aromatic lowercase)\n",
    "            elif char.isalpha():\n",
    "                char_to_atom[i] = atom_idx\n",
    "                # Check for two-letter elements\n",
    "                if i + 1 < len(smiles) and smiles[i + 1].islower():\n",
    "                    char_to_atom[i + 1] = atom_idx\n",
    "                    i += 2\n",
    "                else:\n",
    "                    i += 1\n",
    "                atom_idx += 1\n",
    "                \n",
    "            # Handle bonds\n",
    "            elif char in '=-#:.':\n",
    "                char_to_bond[i] = char\n",
    "                i += 1\n",
    "                \n",
    "            # Handle branches\n",
    "            elif char == '(':\n",
    "                stack.append(atom_idx - 1)  # Push current atom\n",
    "                i += 1\n",
    "            elif char == ')':\n",
    "                if stack:\n",
    "                    stack.pop()\n",
    "                i += 1\n",
    "                \n",
    "            # Handle ring closures\n",
    "            elif char.isdigit():\n",
    "                # Ring closure - this connects to a previous atom\n",
    "                char_to_bond[i] = f'ring_{char}'\n",
    "                i += 1\n",
    "                \n",
    "            # Handle stereochemistry and other symbols\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return mol, char_to_atom, char_to_bond\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in SMILES parsing: {e}\")\n",
    "        return mol, {}, {}\n",
    "\n",
    "def visualize_attribution_scores_improved(subname, smiles, attribution_scores, \n",
    "                                        min_threshold=0.01, debug=False):\n",
    "    \"\"\"\n",
    "    Improved visualization of attribution scores with better mapping\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure attribution scores match SMILES length\n",
    "    if len(attribution_scores) != len(smiles):\n",
    "        print(f\"Warning: Length mismatch - SMILES: {len(smiles)}, Scores: {len(attribution_scores)}\")\n",
    "        # Simple alignment strategy\n",
    "        if len(attribution_scores) < len(smiles):\n",
    "            attribution_scores = list(attribution_scores) + [0.0] * (len(smiles) - len(attribution_scores))\n",
    "        else:\n",
    "            attribution_scores = attribution_scores[:len(smiles)]\n",
    "    \n",
    "    # Get molecule and mappings\n",
    "    mol, char_to_atom, char_to_bond = smiles_to_mol_mapping(smiles)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"SMILES: {smiles}\")\n",
    "        print(f\"Attribution scores: {attribution_scores}\")\n",
    "        print(f\"Char to atom mapping: {char_to_atom}\")\n",
    "        print(f\"Char to bond mapping: {char_to_bond}\")\n",
    "    \n",
    "    # Aggregate scores for atoms\n",
    "    atom_scores = {}\n",
    "    for char_pos, atom_idx in char_to_atom.items():\n",
    "        if char_pos < len(attribution_scores):\n",
    "            if atom_idx not in atom_scores:\n",
    "                atom_scores[atom_idx] = []\n",
    "            atom_scores[atom_idx].append(attribution_scores[char_pos])\n",
    "    \n",
    "    # Average scores for each atom\n",
    "    atom_highlights = {}\n",
    "    atom_radii = {}\n",
    "    \n",
    "    for atom_idx, scores in atom_scores.items():\n",
    "        avg_score = np.mean(scores)\n",
    "        if abs(avg_score) > min_threshold:\n",
    "            atom_highlights[atom_idx] = [score_to_color(avg_score, attribution_scores)]\n",
    "            # Scale radius based on score magnitude\n",
    "            atom_radii[atom_idx] = 0.4 \n",
    "    \n",
    "    # Handle bond highlighting (simplified)\n",
    "    bond_highlights = {}\n",
    "    for char_pos, bond_info in char_to_bond.items():\n",
    "        if char_pos < len(attribution_scores):\n",
    "            score = attribution_scores[char_pos]\n",
    "            if abs(score) > min_threshold:\n",
    "                # For now, we'll skip bond highlighting to focus on atoms\n",
    "                # This is where the complexity lies - mapping character positions to actual bonds\n",
    "                # Map bond characters in SMILES to real bonds\n",
    "                for char_pos, bond_symbol in char_to_bond.items():\n",
    "                    if char_pos < len(attribution_scores):\n",
    "                        score = attribution_scores[char_pos]\n",
    "                        if abs(score) > min_threshold:\n",
    "                            # Try to map bond SMILES position to a real bond\n",
    "                            for bond in mol.GetBonds():\n",
    "                                begin_idx = bond.GetBeginAtomIdx()\n",
    "                                end_idx = bond.GetEndAtomIdx()\n",
    "\n",
    "                                # Map begin/end atom indices to SMILES character positions\n",
    "                                begin_positions = [pos for pos, idx in char_to_atom.items() if idx == begin_idx]\n",
    "                                end_positions = [pos for pos, idx in char_to_atom.items() if idx == end_idx]\n",
    "\n",
    "                                # Check if the bond character is between begin and end in SMILES\n",
    "                                for bpos in begin_positions:\n",
    "                                    for epos in end_positions:\n",
    "                                        if min(bpos, epos) < char_pos < max(bpos, epos):\n",
    "                                            bond_highlights[bond.GetIdx()] = [score_to_color(score, attribution_scores)]\n",
    "                                            break\n",
    "    \n",
    "    # Create the drawing\n",
    "    drawer = rdMolDraw2D.MolDraw2DCairo(400, 400)\n",
    "    drawer.drawOptions().addAtomIndices = debug  # Show atom indices in debug mode\n",
    "    drawer.drawOptions().bondLineWidth = 2\n",
    "    drawer.drawOptions().highlightBondWidthMultiplier = 2\n",
    "    \n",
    "    # Draw molecule with highlights\n",
    "    drawer.DrawMoleculeWithHighlights(\n",
    "        mol,\n",
    "        '', #subname,\n",
    "        atom_highlights,\n",
    "        bond_highlights,\n",
    "        atom_radii,\n",
    "        {}\n",
    "    )\n",
    "    drawer.FinishDrawing()\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    bio = io.BytesIO(drawer.GetDrawingText())\n",
    "    img = Image.open(bio)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Highlighted atoms: {list(atom_highlights.keys())}\")\n",
    "        print(f\"Atom scores: {atom_scores}\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "def create_smiles_attribution_comparison(subname, smiles, attribution_scores, debug=False):\n",
    "    \"\"\"\n",
    "    Create a side-by-side comparison of SMILES attribution and molecular graph\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    \n",
    "    # Left side: SMILES with character-level coloring\n",
    "    ax1.set_title(f'{subname}\\nSMILES Attribution', fontsize=12)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Create character-level visualization\n",
    "    char_colors = [score_to_color(score, attribution_scores) for score in attribution_scores]\n",
    "    \n",
    "    # Display SMILES with colored characters\n",
    "    for i, (char, color) in enumerate(zip(smiles, char_colors)):\n",
    "        ax1.text(i * 0.05, 0.75, char, fontsize=12, ha='center', va='center',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=color, alpha=0.85))\n",
    "    \n",
    "    ax1.set_xlim(-0.05, len(smiles) * 0.05)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Right side: Molecular graph with attribution\n",
    "    ax2.set_title(f'{subname}\\nMolecular Graph', fontsize=12)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Generate molecular graph with attribution\n",
    "    mol_img = visualize_attribution_scores_improved(subname, smiles, attribution_scores, debug=debug)\n",
    "    ax2.imshow(mol_img)\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlBu, \n",
    "                              norm=plt.Normalize(vmin=-1,#min(attribution_scores), \n",
    "                                               vmax=1))#max(attribution_scores)))\n",
    "    sm.set_array([])\n",
    "    #cbar = plt.colorbar(sm, ax=[ax1, ax2], shrink=0.5, aspect=20)\n",
    "    #cbar.set_label('Attribution Score', rotation=270, labelpad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Additional debugging function\n",
    "def debug_attribution_mapping(smiles, attribution_scores):\n",
    "    \"\"\"\n",
    "    Debug function to understand the mapping between SMILES and molecular structure\n",
    "    \"\"\"\n",
    "    mol, char_to_atom, char_to_bond = smiles_to_mol_mapping(smiles)\n",
    "    \n",
    "    print(\"=== ATTRIBUTION MAPPING DEBUG ===\")\n",
    "    print(f\"SMILES: {smiles}\")\n",
    "    print(f\"Length: {len(smiles)}\")\n",
    "    print(f\"Attribution scores: {attribution_scores}\")\n",
    "    print(f\"Scores length: {len(attribution_scores)}\")\n",
    "    print()\n",
    "    \n",
    "    #print(\"Character-by-character analysis:\")\n",
    "    #for i, char in enumerate(smiles):\n",
    "    #    score = attribution_scores[i] if i < len(attribution_scores) else 0.0\n",
    "    #    atom_idx = char_to_atom.get(i, None)\n",
    "    #    bond_info = char_to_bond.get(i, None)\n",
    "        \n",
    "    #    print(f\"  {i:2d}: '{char}' -> Score: {score:6.3f}, Atom: {atom_idx}, Bond: {bond_info}\")\n",
    "    \n",
    "    #print()\n",
    "    #print(\"Atom aggregation:\")\n",
    "    #atom_scores = {}\n",
    "    #for char_pos, atom_idx in char_to_atom.items():\n",
    "    #    if char_pos < len(attribution_scores):\n",
    "    #        if atom_idx not in atom_scores:\n",
    "    #            atom_scores[atom_idx] = []\n",
    "    #        atom_scores[atom_idx].append((char_pos, attribution_scores[char_pos]))\n",
    "    \n",
    "    #for atom_idx, score_list in atom_scores.items():\n",
    "    #    avg_score = np.mean([score for pos, score in score_list])\n",
    "    #    print(f\"  Atom {atom_idx}: positions {[pos for pos, score in score_list]}, \"\n",
    "    #          f\"scores {[score for pos, score in score_list]}, avg: {avg_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34affef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        {\n",
    "            'name': names_list_tve[5],\n",
    "            'smiles': smiles_list_tve[5],\n",
    "            'attribution_scores': expanded_attrs_list_tve[5]\n",
    "        },\n",
    "        {\n",
    "            'name': names_list_tve[7],\n",
    "            'smiles': smiles_list_tve[7],\n",
    "            'attribution_scores': expanded_attrs_list_tve[7]\n",
    "        },\n",
    "        {\n",
    "            'name': names_list_tve[16],\n",
    "            'smiles': smiles_list_tve[16],\n",
    "            'attribution_scores': expanded_attrs_list_tve[16]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing: {test_case['name']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Debug the mapping\n",
    "        debug_attribution_mapping(test_case['smiles'], test_case['attribution_scores'])\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = create_smiles_attribution_comparison(\n",
    "            test_case['name'], \n",
    "            test_case['smiles'], \n",
    "            test_case['attribution_scores'],\n",
    "            debug=False\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3de5a3",
   "metadata": {},
   "source": [
    "## 6. Augment the data by generating alternative SMILES representations\n",
    "\n",
    "We augment the training data by leveraging alternative non-canonical SMILES representaions and retrain the model to see whether this strategy enhances the performance, stability and generalization capabilities of the trained model.\n",
    "We will be generating different non-canonical SMILES representations using the RDKit package.\n",
    "\n",
    "#### Justification\n",
    "SMILES representation: not unique, the same molecule can have multiple valid representations, which translate the same chemical properties in terms of chemical structure and covalent bonds and such, but represent different strings when regarded as text. \n",
    "We enrich the dataset by using this property. We encode the names of the substrates by using different SMILES representations, eliminating the redundant (exactly the same) representations. This logic is supported by the following. \n",
    "\n",
    "\n",
    "#### Treatment of Special Symbols in SMILES\n",
    "In SMILES, special characters represent bonds and molecular structure:\n",
    "\n",
    "* `=` represents a double bond.\n",
    "* `#` represents a triple bond.\n",
    "* `@` indicates stereochemistry (e.g., `@` or `@@` for chirality).\n",
    "* `/` and `\\` indicate the stereochemistry of double bonds (Z/E or cis/trans configurations).\n",
    "\n",
    "When you input a SMILES string (e.g., `C=C`, `C#C`, `C@C`, `C/C=C/C`), the ChemBERTa tokenizer processes it just like any other text. Here's how:\n",
    "\n",
    "Tokenization:\n",
    "\n",
    "The SMILES string is tokenized into sub-units based on the training corpus. ChemBERTa uses a BPE tokenizer to split the input into tokens (for instance, `C`, `=`, `C`, or `C`, `@`, `C` as separate tokens).\n",
    "The tokenizer learns which symbols or combinations of symbols represent meaningful information by seeing many examples of SMILES strings during training.\n",
    "Handling of Special Characters:\n",
    "\n",
    "Special bond characters like `=`, `#`, `@`, `/`, `\\` are treated as part of the tokenization process. They are either represented by their own tokens or grouped with nearby atoms if that's how the tokenizer was trained to split SMILES strings.\n",
    "For instance, in the SMILES string C=C, the tokenizer might treat `C`, `=`, and `C` as separate tokens, while in more complex cases (like stereochemistry with `/` and `\\\\`), tokens might be split differently depending on the specific tokenizer settings.\n",
    "Learned Representations:\n",
    "\n",
    "The model doesn't understand the chemical meaning of these symbols inherently. Instead, it learns from the data to associate certain patterns (e.g., `C=C`) with particular molecular properties or activities through training on labeled datasets (e.g., for classification or regression tasks).\n",
    "ChemBERTa learns embeddings (dense vector representations) for each token, including the ones representing bonds, and then uses these embeddings as part of its predictive mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2359b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(data):\n",
    "\n",
    "    for i in range(10):\n",
    "        random.seed(123*i)\n",
    "\n",
    "        smiles_ = []    \n",
    "        for smiles in data['SMILES'].tolist():\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol: # ensure the molecule object is valid\n",
    "                smiles_.append(Chem.MolToSmiles(mol, canonical=False, doRandom = True))\n",
    "            else:\n",
    "                smiles_.append(None)  # handle invalid SMILES\n",
    "        data[f'SMILES{i+2}'] = smiles_\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92b0c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data, train_ind):\n",
    "    '''\n",
    "    function that splits the extended data into train and test sets and \n",
    "    alters the training set by melting the augmented df to long format and removing redundant SMILES representations\n",
    "    '''\n",
    "    data = data.drop_duplicates()\n",
    "    # replace MVs with 0, if any:\n",
    "    data['Oxd'].fillna(0, inplace=True) \n",
    "    data['Oxd'] = data['Oxd'].round().astype('int64')\n",
    "    # use the trained/test indices from before:\n",
    "    \n",
    "    train_dt, test_dt =  data.loc[train_ind], data.drop(train_ind)\n",
    "    \n",
    "    # prepare the data by augmenting the training set (trnsform wide-to-long, drop redundant SMILES representations) and separating features and labels for training and test sets: \n",
    "    train_dt = pd.melt(train_dt, id_vars=['Oxd', 'Substrate Name', 'IUPAC Name'], value_vars=[s for s in data.columns.tolist() if s.count('SMILES')>0], var_name='SMILES_type')\n",
    "    train_dt = train_dt.drop(['Substrate Name', 'IUPAC Name', 'SMILES_type'], axis=1)\n",
    "    train_dt = train_dt.drop_duplicates()\n",
    "\n",
    "    train_data, train_labels = train_dt['value'].rename('SMILES'), train_dt['Oxd']\n",
    "    test_data, test_labels = test_dt['SMILES'], test_dt['Oxd'] # disregard alternative representations for the test data  \n",
    "    \n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d39f736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples (f-tve): 2067\n",
      "Number of test examples (f-tve): 50\n",
      "\n",
      "Number of training examples (f-mth) 1560\n",
      "Number of test examples (f-mth): 38\n",
      "\n",
      "Number of training examples (bpu-lac) 1392\n",
      "Number of test examples (bpu-lac): 34\n"
     ]
    }
   ],
   "source": [
    "# extend the datasets:\n",
    "data_tve_ = data_tve.copy()\n",
    "augment(data_tve_)\n",
    "train_data_tve_, test_data_tve_, train_labels_tve_, test_labels_tve_ = data_split(data_tve_, train_data_tve.index.to_list()) \n",
    "\n",
    "data_mth_ = data_mth.copy()\n",
    "augment(data_mth_)\n",
    "train_data_mth_, test_data_mth_, train_labels_mth_, test_labels_mth_ = data_split(data_mth_, train_data_mth.index.to_list()) \n",
    "\n",
    "data_bpu_ = data_bpu.copy()\n",
    "augment(data_bpu_)\n",
    "train_data_bpu_, test_data_bpu_, train_labels_bpu_, test_labels_bpu_ = data_split(data_bpu_, train_data_bpu.index.to_list()) \n",
    "\n",
    "# display set sizes:\n",
    "print(\"Number of training examples (f-tve):\", len(train_data_tve_))\n",
    "print(\"Number of test examples (f-tve):\", len(test_data_tve_))\n",
    "print('\\nNumber of training examples (f-mth)', len(train_data_mth_))\n",
    "print(\"Number of test examples (f-mth):\", len(test_data_mth_))\n",
    "print('\\nNumber of training examples (bpu-lac)', len(train_data_bpu_))\n",
    "print(\"Number of test examples (bpu-lac):\", len(test_data_bpu_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647ca40",
   "metadata": {},
   "source": [
    "### Retrain and rebuild the model on the augmented data (for f-tve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39717d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_tve_ = tune_model(train_data_tve_, train_labels_tve_)\n",
    "final_model_tve_, logits_tve_, attentions_tve_, h_states_tve_, inputs_tve_, mask_tve_, preds_tve_, true_labels_tve_ = rebuild_model(train_data_tve_, train_labels_tve_, test_data_tve_, test_labels_tve_, study_tve_)\n",
    "evaluate_model(true_labels_tve_, preds_tve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a8b5c",
   "metadata": {},
   "source": [
    "## Extras: \n",
    "\n",
    "### Training the model with frozen layers: \n",
    "\n",
    "* RoBERTa is frozen  requires_grad = False\n",
    "* only the classifier (self.classifier) is trainable\n",
    "* with torch.no_grad() is used for inference in frozen layers\n",
    "* optimizer updated to only train classifier parameters\n",
    "\n",
    "```python\n",
    "\n",
    "class CustomRoberta(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout_prob):\n",
    "        \n",
    "        super(CustomRoberta, self).__init__()\n",
    "        self.checkpoint = 'seyonec/PubChem10M_SMILES_BPE_450k' \n",
    "        self.roberta = RobertaModel.from_pretrained(self.checkpoint, output_attentions=True, output_hidden_states=True).to(device)\n",
    "        \n",
    "        # freeze all RoBERTa layers:\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.config = self.roberta.config \n",
    "        self.current_embeddings = self.roberta.get_input_embeddings()\n",
    "        self.tokenizer = RobertaTokenizerFast.from_pretrained(self.checkpoint)\n",
    "        self.dropout = nn.Dropout(dropout_prob).to(device)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, 2).to(device)\n",
    "        \n",
    "        \n",
    "        # unfreeze the classification head of the RoBERTa model:\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "       \n",
    "    def prepare_data(self, data, labels, batch_size, shuffle):\n",
    "        \n",
    "        # tokenize the SMILES:\n",
    "        tokenized = self.tokenizer(data.tolist(), padding=True, truncation=True, max_length=180, return_tensors='pt')\n",
    "        # extract input ids and attention masks:\n",
    "        input_ids = tokenized['input_ids']\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "        # convert labels to tensors:\n",
    "        labels = torch.tensor(labels.values)\n",
    "        # create DataLoaders for batching:\n",
    "        dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, worker_init_fn=lambda _: np.random.seed(42))\n",
    "        return loader#, input_ids, attention_mask    \n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs[1]  # [CLS] token representation\n",
    "        \n",
    "        dropped_output = self.dropout(pooled_output)  # add dropout\n",
    "        logits = self.classifier(dropped_output)  # pass through classification layer\n",
    "        \n",
    "        return logits, outputs.attentions, outputs.hidden_states\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ff43d",
   "metadata": {},
   "source": [
    "### Data diversity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a723c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from scipy.stats import iqr\n",
    "from sklearn.metrics import jaccard_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "\n",
    "# --- 0. Sample Dataset ---\n",
    "\n",
    "df = pd.DataFrame(data_tve)\n",
    "df['Molecule'] = df['SMILES'].apply(Chem.MolFromSmiles)\n",
    "\n",
    "\n",
    "def get_group(name: str) -> str:\n",
    "    if name in {'Trans-cinnamic acid', 'm-Coumaric acid', 'p-Coumaric acid', 'o-Coumaric acid', 'Caffeic acid', 'Ferulic acid',\n",
    "                'Sinapic acid', '4-Hydroxybenzoic acid', 'Salicylic acid', '4-Methoxybenzoic acid', '3-Methoxybenzoic acid',\n",
    "                '2-Methoxybenzoic acid', '3,4-Dihydroxybenzoic acid', 'Gallic acid', 'Syringic acid', '4-Aminobenzoic acid',\n",
    "                '3-Aminobenzoic acid', '3-Amino-4-hydroxybenzoic acid', '4-Amino-3-hydroxybenzoic acid', 'Vanillic acid',\n",
    "                'Anthranilic acid', '3-Hydroxyanthranilic acid', '4-Aminosalicylic acid', '2,6-Dihydroxybenzoic acid'}:\n",
    "        return 'Aromatic Carboxylic Acids'\n",
    "    elif name in {'4-Hydroxybenzyl alcohol', 'Vanillyl alcohol', 'Isovanillyl alcohol', '2,3-Dimethoxybenzyl alcohol',\n",
    "                  '2,4-Dimethoxybenzyl alcohol', '2,5-Dimethoxybenzyl alcohol', 'Veratryl alcohol', '3,5-Dimethoxybenzyl alcohol',\n",
    "                  'Coniferyl alcohol', 'Tyrosol', '3-Hydroxyphenethyl alcohol', '2-Hydroxyphenethyl alcohol', 'Phenol',\n",
    "                  'p-Cresol', 'o-Cresol', 'm-Cresol', '2,6-Dimethylphenol', '2,4-Dimethylphenol', '3,4-Dimethylphenol',\n",
    "                  '3,5-Dimethylphenol', 'Catechol (o-Hydroquinone)', 'Resorcinol', 'Hydroquinone (p-Hydroquinone)',\n",
    "                  '4-Methylcatechol', 'Pyrogallol', '3,4,5-Trimethoxyphenol', 'Guaiacol (2-methoxyphenol)',\n",
    "                  '4-Methoxyphenol', '3-Methoxyphenol', 'Mesitol (2,4,6-Trimethylphenol)', '3-Methylcatechol',\n",
    "                  'Eugenol', '4-Ethylphenol', '3-Ethylphenol', '2-Ethylphenol', '4-Ethoxyphenol', '3-Ethoxyphenol',\n",
    "                  '2-Ethoxyphenol', '2-methoxy-4-methylphenol', 'Thymol', '4-Aminophenol', '3-Aminophenol', '2-Aminophenol'}:\n",
    "        return 'Aromatic Alcohols'\n",
    "    elif name in {'Acetovanillone', 'Acetosyringone', '4-Hydroxyacetophenone', '3-Hydroxyacetophenone', '2-Hydroxyacetophenone',\n",
    "                  \"4'-Aminoacetophenone\", \"3'-Aminoacetophenone\", \"2'-Aminoacetophenone\", 'Butyrophenone',\n",
    "                  \"2'-Methylacetophenone\", \"3'-Methylacetophenone\", \"4'-Methylacetophenone\"}:\n",
    "        return 'Aromatic Ketones'\n",
    "    elif name in {'o-Vanillin', 'Syringaldehyde', 'Ethyl vanillin', 'Vanillin', 'Sinapaldehyde', 'Coniferyl aldehyde',\n",
    "                  'Benzaldehyde', 'Cinnamaldehyde', 'p-Tolualdehyde', 'm-Tolualdehyde', 'o-Tolualdehyde', 'Cuminaldehyde',\n",
    "                  'p-Anisaldehyde', 'm-Anisaldehyde', 'o-Anisaldehyde', '4-Hydroxybenzaldehyde', '3-Hydroxybenzaldehyde',\n",
    "                  '2-Hydroxybenzaldehyde', '2,4-Dihydroxybenzaldehyde'}:\n",
    "        return 'Aromatic Aldehydes'\n",
    "    elif name in(['Aniline', '2-Phenethylamine', 'Tyramine', 'Dopamine', 'p-Toluidine (4-Methylaniline)', 'm-Toluidine (3-Methylaniline)', \n",
    "                   'o-Toluidine (2-Methylaniline)', '2,4-Dimethylaniline', '3,5-Dimethylaniline', '2,6-Diethylaniline', '2,6-Dimethylaniline', \n",
    "                   '2,5-Dimethylaniline', '4-Ethylaniline', '3-Ethylaniline', '2-Ethylaniline', 'p-Anisidine (4-Methoxyaniline)', \n",
    "                   'm-Anisidine (3-Methoxyaniline)', 'o-Anisidine (2-Methoxyaniline)', '4-Ethoxyaniline', '3-Ethoxyaniline', '2-Ethoxyaniline']): \n",
    "        return 'Aromatic Amines' \n",
    "    elif name in(['Methyl vanillate', 'Methyl syringate', 'Methyl 4-hydroxybenzoate', 'Methyl 3-hydroxybenzoate', 'Methyl salicylate', \n",
    "                  'Benzyl acetate']): \n",
    "        return 'Aromatic Esters' \n",
    "    elif name in(['Syringamide', '4-Methoxybenzamide', '3-Methoxybenzamide', 'Anthranilamide', '3-Aminobenzamide', \n",
    "                    '4-Aminobenzamide', '2-Methylbenzamide', '4-Methylbenzamide']): \n",
    "        return 'Aromatic Amides' \n",
    "    elif name in(['3-Fluoro-4-hydroxybenzoic acid', '3-Dimethylaminobenzoic acid', 'Sodium salicylate', 'p-Hydroxyphenylpyruvic acid', 'L-DOPA']): \n",
    "        return 'Aromatic Carboxylic Acids' \n",
    "    elif name in(['Isoeugenol', 'Arbutin', 'Resveratrol', 'Quercetin hydrate', '2,6-Dimethoxyphenol']): \n",
    "        return 'Aromatic Alcohols' \n",
    "    elif name in(['N-Hydroxyacetanilide']): \n",
    "        return 'Aromatic Amines' \n",
    "    elif name in(['HOBt', 'N-Hydroxyphthalimide', 'HOAt', 'DHBT', 'Violuric acid hydrate', 'TEMPO', 'TEMPOL', '3-Carbamoyl-PROXYL', \n",
    "                  '1-(3-Sulfophenyl)-3-methyl-2-pyrazolin-5-one', '1-(4-Sulfophenyl)-3-methyl-5-pyrazolone', 'Methyl viologen dichloride hydrate']): \n",
    "        return 'N-Heterocycles' \n",
    "    elif name in(['ABTS', 'Syringaldazine']): \n",
    "        return 'Aromatic Azo Compounds' \n",
    "    elif name in(['Phenolphthalein', 'Triphenylamine', 'Phenol red', 'Cresol red sodium salt']): \n",
    "        return 'Triphenyl Compounds' \n",
    "    elif name in(['(+)-Catechin hydrate', '(-)-Epicatechin']): \n",
    "        return 'Chroman' \n",
    "    elif name in(['Phenothiazine', 'Promazine hydrochloride']): \n",
    "        return 'Phenothiazines' \n",
    "    elif name in(['2,3-Dimethoxybenzonitrile', '3,5-Dimethoxybenzonitrile']): \n",
    "        return 'Benzonitriles' \n",
    "    elif name in(['1-Nitroso-2-naphthol-3,6-disulfonic acid', '2-Nitroso-1-naphthol-4-sulfonic acid', '1-Amino-2-naphthol-4-sulfonic acid', \n",
    "                  '3-amino-N-Hydroxyphthalimide', '4-methyl-N-Hydroxyphthalimide', '3,4-dimethoxy-N-Hydroxyphthalimide', '3,5-dimethyl-N-Hydroxyphthalimide', \n",
    "                  '4-nitro-N-Hydroxyphthalimide', \"N,N'-Dihydroxypyromellitimide\", 'Reactive black 5', 'Anthracene', 'Acenaphthylene', 'Acenaphthene', 'Benzo(a)pyrene', \n",
    "                  '4-butylaniline', '2-butylphenol', '2,4-Dichlorophenol', 'trans-caffeic acid', 'Hydroxytyrosol', '3-Hydroxybenzoic acid', '3-hydroxyphenylacetic acid', \n",
    "                  \"2',6-Dihydroxybenzoic acids\", '3-(4-Hydroxyphenyl)-1,1-dimethylurea', '3-(2-Hydroxyphenyl)-1,1-dimethylurea', '3-(3-Hydroxyphenyl)-1,1-dimethylurea', \n",
    "                  'Triclosan', '4-Nonylphenol', 'Estrone', 'Genistein', 'Estriol', 'Paracetamol', 'Naproxen', 'Mefenamic acid', 'Sulfanilamide', 'Sulfadimethoxine', \n",
    "                  'Sulfapyridine', 'Bisphenol F', 'Bisphenol S', 'Bisphenol A', 'm-Chlorophenol', 'L-Tyrosine', 'D-Tyrosine', '2,4,6-Trichlorophenol', 'Thiodicarb', \n",
    "                  'Malathion', 'Captan', 'Atrazine', 'Promazine', 'Chlorpromazine', '2-chloroaniline', '3-chloroaniline', '4-chloroaniline', '2,3-dichloroaniline', \n",
    "                  '3,4-dichloroaniline', '2-ethyl-6-methylaniline', '4-chloro-o-toluidine', '4-methylamino benzoic acid', '(2-Amino-3-hydroxyphenyl)phosphonic acid', \n",
    "                  '2-amino-3-hydroxybenzenesulfonamide', '2-amino-N-cyclohexyl-3-hydroxybenzenesulfonamide', '2-amino-N-(3-(dimethylamino)propyl)-3-hydroxybenzenesulfonamide', \n",
    "                  '2-amino-3-hydroxy-N-phenylbenzenesulfonamide', '2-amino-N-(2-aminoethyl)-3-hydroxybenzenesulfonamide', 'methyl ((2-amino-3-hydroxyphenyl)sulfonyl)glycinate', \n",
    "                  '2-amino-3-hydroxy-N-(2-hydroxyethyl)benzenesulfonamide', '3-amino-N-cyclohexyl-2-hydroxybenzenesulfonamide', \n",
    "                  '3-amino-N-(3-(dimethylamino)propyl)-2-hydroxybenzenesulfonamide', '3-amino-2-hydroxybenzenesulfonic acid', '3-Hydroxyorthanilic acid', \n",
    "                  '1-amino-4-(cyclohexylamino)-9,10-dioxo-9,10-dihydroanthracene-2-sulfonic acid', '4-methyl-3-hydroxyanthranilic acid', '4-Ethyl-1,2,3-benzenetriol', \n",
    "                  '4-Propyl-1,2,3-benzenetriol', '4-Butyl-1,2,3-benzenetriol', '4-Dodecyl-1,2,3-benzenetriol', '4-(octan-3-yl)benzene-1,2,3-benzenetriol', \n",
    "                  'ethyl 4-(2,3,4-trihydroxyphenyl)butanoate', '4-(3-Hydroxypropyl)-1,2,3-benzenetriol', '3-(2,3,4-Trihydroxyphenyl)propanoic acid', \n",
    "                  '4-(3-(2,3,4-trihydroxyphenyl)propanamido)butanoic acid', 'N-octyl-3-(2,3,4-trihydroxyphenyl)propanamide', '2-(3-(2,3,4-trihydroxyphenyl)propanamido)ethane-1-sulfonic acid', \n",
    "                  'N-(2,3,4,5,6-pentahydroxyhexyl)-3-(2,4-trihydroxyphenyl)propanamide', 'N-phenyl-1,4-phenylenediamine' ]): \n",
    "        return 'Naphthalenes'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# apply to the column that has the compound names\n",
    "df['Group'] = df['Substrate Name'].apply(get_group) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bemis-Murcko scaffold analysis\n",
    "print(\"(i) Bemis-Murcko scaffold analysis:\")\n",
    "scaffolds = {}\n",
    "for index, row in df.iterrows():\n",
    "    mol = row['Molecule']\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    scaffold_smiles = Chem.MolToSmiles(scaffold)\n",
    "    if scaffold_smiles:  # Ensure scaffold is not empty\n",
    "        scaffolds.setdefault(scaffold_smiles, []).append(row['Substrate Name'])\n",
    "\n",
    "scaffold_counts = pd.Series({k: len(v) for k, v in scaffolds.items()}).sort_values(ascending=False)\n",
    "num_scaffolds = len(scaffold_counts)\n",
    "num_singletons = (scaffold_counts == 1).sum()\n",
    "singleton_percentage = (num_singletons / num_scaffolds) * 100 if num_scaffolds > 0 else 0\n",
    "\n",
    "print(f\"Total unique scaffolds: {num_scaffolds}\")\n",
    "print(f\"Singleton scaffolds: {num_singletons} ({singleton_percentage:.2f}%)\")\n",
    "\n",
    "# Top-k scaffold coverage (e.g., k=3)\n",
    "k = 3\n",
    "top_k_scaffolds = scaffold_counts.head(k)\n",
    "coverage = (top_k_scaffolds.sum() / len(df)) * 100\n",
    "print(f\"\\nTop-{k} Scaffolds cover {coverage:.2f}% of the dataset:\")\n",
    "print(top_k_scaffolds)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# 2. ECFP4 fingerprint & Tanimoto similarity analysis\n",
    "print(\"\\n(ii) ECFP4 Tanimoto similarity analysis:\")\n",
    "# Generate ECFP4 fingerprints\n",
    "df['ECFP4'] = [AllChem.GetMorganFingerprintAsBitVect(m, radius=2, nBits=2048) for m in df['Molecule']]\n",
    "\n",
    "# Calculate pairwise Tanimoto similarities\n",
    "similarity_matrix = []\n",
    "for i in range(len(df)):\n",
    "    similarities = DataStructs.BulkTanimotoSimilarity(df['ECFP4'][i], df['ECFP4'].tolist())\n",
    "    # Exclude self-similarity\n",
    "    similarities.pop(i)\n",
    "    similarity_matrix.extend(similarities)\n",
    "\n",
    "similarities_flat = np.array(similarity_matrix)\n",
    "\n",
    "mean_tanimoto = np.mean(similarities_flat)\n",
    "median_tanimoto = np.median(similarities_flat)\n",
    "iqr_tanimoto = iqr(similarities_flat)\n",
    "internal_diversity = 1 - mean_tanimoto\n",
    "\n",
    "print(f\"Mean Tanimoto similarity: {mean_tanimoto:.3f}\")\n",
    "print(f\"Median Tanimoto similarity: {median_tanimoto:.3f}\")\n",
    "print(f\"IQR of Tanimoto similarity: {iqr_tanimoto:.3f}\")\n",
    "print(f\"Internal diversity (1 - mean_Tanimoto): {internal_diversity:.3f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# 3. Nearest-neighbor Tanimoto distribution\n",
    "print(\"\\n(iii) Nearest-neighbor Tanimoto analysis\")\n",
    "nearest_neighbors = []\n",
    "for i in range(len(df)):\n",
    "    # Calculate similarities to all others\n",
    "    sims = DataStructs.BulkTanimotoSimilarity(df['ECFP4'][i], df['ECFP4'].tolist())\n",
    "    # Set self-similarity to 0 to easily find the max\n",
    "    sims[i] = 0\n",
    "    nearest_neighbors.append(max(sims))\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(nearest_neighbors, kde=True, bins=20)\n",
    "plt.title('Distribution of Nearest-Neighbor Tanimoto Similarities')\n",
    "plt.xlabel('Tanimoto Similarity to Nearest Neighbor')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('nearest_neighbor_distribution.png')\n",
    "print(\"Plot saved as 'nearest_neighbor_distribution.png'.\")\n",
    "print(\"This plot shows local redundancy. A skew towards 1.0 indicates high redundancy.\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# 4. Dataset overlap via Jaccard index of scaffolds\n",
    "print(\"\\nAssessing dataset overlap via Jaccard index\")\n",
    "# Get scaffold sets for each enzyme\n",
    "enzyme_scaffolds = {}\n",
    "for g in df['Group'].unique():\n",
    "    subset_df = df[df['Group'] == g]\n",
    "    scaffold_set = set()\n",
    "    for mol in subset_df['Molecule']:\n",
    "        scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "        scaffold_smiles = Chem.MolToSmiles(scaffold)\n",
    "        if scaffold_smiles:\n",
    "            scaffold_set.add(scaffold_smiles)\n",
    "    enzyme_scaffolds[g] = scaffold_set\n",
    "\n",
    "# Calculate pairwise Jaccard index\n",
    "enzymes = list(enzyme_scaffolds.keys())\n",
    "jaccard_matrix = pd.DataFrame(index=enzymes, columns=enzymes, dtype=float)\n",
    "\n",
    "for i in range(len(enzymes)):\n",
    "    for j in range(len(enzymes)):\n",
    "        set1 = enzyme_scaffolds[enzymes[i]]\n",
    "        set2 = enzyme_scaffolds[enzymes[j]]\n",
    "        jaccard_index = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "        jaccard_matrix.iloc[i, j] = jaccard_index\n",
    "\n",
    "print(\"Jaccard index matrix of scaffold Sets:\")\n",
    "print(jaccard_matrix.round(3))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# 5. UMAP projection of chemical space\n",
    "print(\"\\n--- Visualization of chemical space with UMAP ---\")\n",
    "# Convert fingerprints to a numpy array\n",
    "fp_array = np.array([list(fp) for fp in df['ECFP4']])\n",
    "\n",
    "\n",
    "# UMAP projection\n",
    "reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, random_state=42)\n",
    "embedding = reducer.fit_transform(fp_array)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=df['Group'], s=150, alpha=0.8, palette='RdYlBu')\n",
    "plt.title('UMAP Projection of Chemical Space (ECFP4)', fontsize=14)\n",
    "plt.xlabel('UMAP Dim 1')\n",
    "plt.ylabel('UMAP Dim 2')\n",
    "plt.legend(title='Enzyme Group')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.savefig('umap_projection.png')\n",
    "print(\"Plot saved as 'umap_projection.png'.\")\n",
    "print(\"This plot visualizes the chemical space, with colors indicating the enzyme group.\")\n",
    "print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lacenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
